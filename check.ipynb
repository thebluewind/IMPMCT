{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "校验数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import cartopy.crs as ccrs\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.patches as patches\n",
    "import cmaps\n",
    "import cartopy.feature as cfeature\n",
    "from datetime import datetime,timedelta\n",
    "from matplotlib.offsetbox import AnchoredText#关于修改text位置的\n",
    "import cv2\n",
    "from pygac import get_reader_class\n",
    "import ast\n",
    "from Five_utiles4 import *\n",
    "from tqdm import tqdm\n",
    "proj =ccrs.NorthPolarStereo(central_longitude=17.5)\n",
    "from sklearn.neighbors import BallTree\n",
    "from shapely.geometry import Point, Polygon,LineString\n",
    "#经过实验，按经纬度插值的清晰度小于先转化为实际地理坐标系再插值的清晰度\n",
    "import concurrent.futures\n",
    "import pyproj\n",
    "import matplotlib.style as mplstyle\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from shapely.geometry import Polygon, Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_meantime(filepath,tle_name,central_lon,central_lat):\n",
    "    #计算中心点50KM内的平均扫描时间\n",
    "    tle_dir = r'Z:\\NOAA_TLE'\n",
    "    reader_cls = get_reader_class(filepath)\n",
    "    reader = reader_cls(tle_dir=tle_dir, tle_name=tle_name)\n",
    "    reader.read(filepath)\n",
    "    times = reader.get_times()\n",
    "    lons, lats = reader.get_lonlat()\n",
    "    dis_array = geodistance(central_lon,central_lat,lons,lats)\n",
    "    scantime150 = times[(dis_array<50).sum(axis=1)>0]\n",
    "    #去除空值\n",
    "    scantime150 = scantime150[~np.isnan(scantime150)]\n",
    "    if len(scantime150)==0:\n",
    "        return np.nan\n",
    "    meantime = pd.Series(scantime150).mean()\n",
    "    #规整到分钟\n",
    "    return pd.to_datetime( meantime.strftime('%Y%m%d_%H%M'),format='%Y%m%d_%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将json存储为yolo格式的标签\n",
    "def json_to_yolo_obb_pose(load_dict):\n",
    "    class_name = {'comma':0,'spiral':1}\n",
    "    #由于都是正方形所有只需要读图片高度即可\n",
    "    #标注的框\n",
    "    shapes = load_dict['shapes']\n",
    "    boxes,points = [],[]\n",
    "    box_column  = ['group_id','label','x1','y1','x2','y2','x3','y3','x4','y4']\n",
    "    point_column = ['group_id','x0','y0','is_visible']\n",
    "    for idx,temp in enumerate(shapes):\n",
    "        if temp['shape_type']=='point':\n",
    "            points.append([temp['group_id']]+np.round(np.array(temp['points']).flatten(),3).tolist()+[2])\n",
    "        else: \n",
    "            boxes.append([temp['group_id'],temp['label']]+np.round(np.array(temp['points']).flatten(),3).tolist())\n",
    "    box_df = pd.DataFrame(boxes,columns=box_column).set_index('group_id')\n",
    "    point_df = pd.DataFrame(points,columns=point_column).set_index('group_id')\n",
    "    #检验标签的group_id\n",
    "    flag = np.array_equal(np.sort(box_df.index.values),np.sort(point_df.index.values))\n",
    "    if np.sum(~np.isin(box_df.label,['comma','spiral']))!=0:\n",
    "        flag=False\n",
    "    if flag:\n",
    "        #当两者没有共同group_id时会为空\n",
    "        result = pd.DataFrame(boxes,columns=box_column).set_index('group_id').join(pd.DataFrame(points,columns=point_column).set_index('group_id'),how='inner')\n",
    "    else:\n",
    "        print('%s:error'%load_dict['imagePath'])\n",
    "        return []\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosed_columns = ['point_index', 'time','lon', 'lat','split_path_index', 'filepath','tle_name', 'dataset_type', 'satellite_name','filename','mean_time']\n",
    "ERA5_matched = pd.read_excel(r\"D:\\Desktop\\IMPMCT\\new_ERA5_matched.xlsx\",usecols=chosed_columns).reset_index(drop=True)\n",
    "folders = os.listdir(r'D:\\Desktop\\IMPMCT\\tracks')\n",
    "json_forder = r'D:\\Desktop\\IMPMCT\\cloud_jsons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1185/1185 [00:48<00:00, 24.65it/s]\n"
     ]
    }
   ],
   "source": [
    "ID = 1\n",
    "tracks,clouds = [],[]\n",
    "for folder in tqdm(folders):\n",
    "    #读取track\n",
    "    xlsx_files = glob.glob(os.path.join(r'D:\\Desktop\\IMPMCT\\tracks',folder, \"*.xlsx\"))\n",
    "    if len(xlsx_files)!=1:\n",
    "        print(folder)\n",
    "        continue\n",
    "    track_path = xlsx_files[0]\n",
    "    #当轨迹是新轨迹时\n",
    "    if os.path.basename(track_path)[0]=='n':\n",
    "        track = pd.read_excel(track_path,header=None)\n",
    "        track.columns = ['point_index','time','path_index','peak_value','lon','lat','split_path_index','relative']\n",
    "    else:\n",
    "        track = pd.read_excel(track_path)\n",
    "    track['ID'] = ID\n",
    "    tracks.append(track)\n",
    "    #读取clouds文件\n",
    "    infos = []\n",
    "    cloud_folder = os.path.join(r'D:\\Desktop\\IMPMCT\\core',folder)\n",
    "    for file in os.listdir(cloud_folder):\n",
    "        #避免选到json文件\n",
    "        if file[-1]=='n':\n",
    "            continue\n",
    "        json_path = os.path.join(json_forder,file.replace('.png','.json'))\n",
    "        with open(json_path,'r', encoding='UTF-8') as f:    \n",
    "            load_dict = json.load(f)\n",
    "        info = json_to_yolo_obb_pose(load_dict)\n",
    "        info['filename'] = file\n",
    "        infos.append(info)\n",
    "    cloud_info = pd.concat(infos,axis=0)\n",
    "    cloud_info['ID'] = ID\n",
    "    clouds.append(cloud_info)\n",
    "    ID+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo2lonlat(central_lon,central_lat,distance_x,distance_y):\n",
    "    #位移不变的经纬度转换\n",
    "    earth_radius = 6370856#m\n",
    "    lats = distance_y/(2*np.pi*earth_radius)*360+central_lat\n",
    "    lon_diff = distance_x/(2*np.pi*earth_radius*np.cos(central_lat/180*np.pi))*360\n",
    "    return central_lon+lon_diff,lats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERA5_track,cloud_infos = pd.concat(tracks,axis=0),pd.concat(clouds,axis=0)\n",
    "cloud_infos = pd.merge(cloud_infos,ERA5_matched,on=['filename'],how='left')\n",
    "kpt_lon,kpt_lat = geo2lonlat(cloud_infos.lon,cloud_infos.lat,(cloud_infos.x0-401)*2e3,(401-cloud_infos.y0)*2e3)\n",
    "cloud_infos['cloud_lon'],cloud_infos['cloud_lat'] = kpt_lon,kpt_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 327/1185 [00:00<00:01, 531.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轨迹16926多出4个点\n",
      "轨迹185平均距离194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 728/1185 [00:01<00:00, 559.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轨迹27631多出4个点\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 957/1185 [00:01<00:00, 556.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轨迹40614多出6个点\n",
      "轨迹42934平均距离150\n",
      "轨迹442多出4个点\n",
      "轨迹507多出4个点\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1185/1185 [00:02<00:00, 545.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#判断cloud轨迹与track轨迹的重合度\n",
    "for chosed_ID in tqdm(ERA5_track.ID.unique()):\n",
    "    chosed_track = ERA5_track[ERA5_track['ID']==chosed_ID]\n",
    "    chosed_cloud_info = cloud_infos[cloud_infos['ID']==chosed_ID]\n",
    "    track_point_index = np.unique(chosed_track.point_index)\n",
    "    cloud_point_index = np.unique(chosed_cloud_info.point_index)\n",
    "    #cloud_info中的点在track中的比例\n",
    "    more_num = len(cloud_point_index)-np.isin(cloud_point_index,track_point_index).sum()\n",
    "    if more_num>3:\n",
    "        print('轨迹%d多出%d个点'%(np.unique(chosed_track.path_index[0]),more_num))\n",
    "    #粗略计算重叠点的平均距离\n",
    "    overlap_cloud = chosed_cloud_info.loc[np.isin(chosed_cloud_info.point_index,track_point_index)]\n",
    "    overlap_cloud['dis'] = geodistance(overlap_cloud.lon,overlap_cloud.lat,overlap_cloud.cloud_lon,overlap_cloud.cloud_lat)\n",
    "    mean_dis = overlap_cloud.dis.mean()\n",
    "    if mean_dis>150:\n",
    "        print('轨迹%d平均距离%d'%(np.unique(chosed_track.path_index[0]),mean_dis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "判断是否存在云特征被重复检测的场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15046/15046 [00:15<00:00, 953.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cloud_infos['AVHRR_filename'] = cloud_infos['filepath'].apply(lambda x:os.path.basename(x))\n",
    "unique_filename = np.unique(cloud_infos['AVHRR_filename'])\n",
    "overfind = []\n",
    "for i in tqdm(range(len(unique_filename))):\n",
    "    temp_info = cloud_infos.loc[cloud_infos.AVHRR_filename==unique_filename[i]].copy()\n",
    "    all_index = temp_info.index\n",
    "    while len(all_index)>1:\n",
    "        central_lon,central_lat = temp_info.loc[all_index[0],['cloud_lon','cloud_lat']].values\n",
    "        #小于50KM时认为是一个,cluster至少含有一个点就是自己\n",
    "        distancess = geodistance(central_lon,central_lat,temp_info.loc[all_index,'cloud_lon'].values,temp_info.loc[all_index,'cloud_lat'].values)\n",
    "        cluster = temp_info.loc[all_index[distancess<=50]]\n",
    "        all_index = all_index[distancess>50]\n",
    "        if len(cluster)>1:\n",
    "            overfind.append(cluster)\n",
    "print(len(overfind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计每个ID的cloud时间\n",
    "IDs = cloud_infos.ID.unique()\n",
    "for i in range(len(IDs)):\n",
    "    chosed_cloud = cloud_infos.loc[cloud_infos['ID']==IDs[i]]\n",
    "    durhour = (chosed_cloud.mean_time.max()-chosed_cloud.mean_time.min()).total_seconds()/3600\n",
    "    cloud_num = len(chosed_cloud)\n",
    "    if durhour<4:\n",
    "        print('ID:%d 持续时间%.2fh'%(IDs[i],durhour))\n",
    "    if cloud_num<4:\n",
    "        print('ID:%d 气旋%d个'%(IDs[i],cloud_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16418it [1:48:42,  2.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#气旋中心精确时间计算\n",
    "cloud_times = []\n",
    "for index,cloud_info in tqdm(cloud_infos.iterrows()):\n",
    "    cloud_times.append(calculate_meantime(cloud_info.filepath,cloud_info.tle_name,cloud_info.cloud_lon,cloud_info.cloud_lat))\n",
    "cloud_infos['cloud_time'] = cloud_times  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同化其他信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_info_folder = r'E:\\final_tracks\\tracks_monthly_area'\n",
    "track_infos = pd.concat([pd.read_excel(os.path.join(track_info_folder,i)).drop(columns=['Unnamed: 0','peak_value']) for i in os.listdir(track_info_folder)])\n",
    "#一些错误边界的轨迹号\n",
    "error_split_path_index = pd.read_csv(r\"D:\\Desktop\\IMPMCT\\error_area.txt\",header=None).loc[:,0].values\n",
    "ERA5_tracks_infos = pd.merge(ERA5_track,track_infos,on=['point_index','lon','lat','time','split_path_index','path_index'],how='inner')\n",
    "error_area_bool = np.isin(ERA5_tracks_infos.split_path_index,error_split_path_index)\n",
    "ERA5_tracks_infos.loc[error_area_bool,'isolated_area_path'] = ERA5_tracks_infos.loc[error_area_bool,'vort_min_line']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照起始时间重排列ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_dict = ERA5_tracks_infos.groupby('ID')['time'].min().reset_index().sort_values(['time'])\n",
    "ID_dict['new_ID'] = np.arange(len(ID_dict))+1\n",
    "ERA5_tracks_infos['ID'] = ERA5_tracks_infos['ID'].replace(ID_dict.ID.values,ID_dict.new_ID.values)\n",
    "cloud_infos['ID'] = cloud_infos['ID'].replace(ID_dict.ID.values,ID_dict.new_ID.values)\n",
    "cloud_infos = cloud_infos.sort_values(by=['ID','cloud_time']).reset_index(drop=True)\n",
    "ERA5_tracks_infos = ERA5_tracks_infos.sort_values(by=['ID','time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#当每一个轨迹开始时，有邻接的涡旋认为是分裂\n",
    "#当每一个轨迹结束时，有邻接的涡旋认为是合并\n",
    "track_start_indexs = ERA5_tracks_infos.groupby('ID')['time'].idxmin()\n",
    "track_start = ERA5_tracks_infos.loc[track_start_indexs]\n",
    "split_IDs = track_start.loc[track_start.isolated_area_path!=track_start.original_vort_line]\n",
    "#-------------------------------------------------------------------------------------------\n",
    "track_end_indexs = ERA5_tracks_infos.groupby('ID')['time'].idxmax()\n",
    "track_end = ERA5_tracks_infos.loc[track_end_indexs]\n",
    "merge_IDs = track_end.loc[track_end.isolated_area_path!=track_end.original_vort_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存分裂与合并的ID\n",
    "np.savetxt(r'D:\\Desktop\\final_dataset\\IMPMCT\\auxiliary information\\split_IDs.txt', split_IDs.ID.values.reshape(-1, 1), fmt='%d')\n",
    "np.savetxt(r'D:\\Desktop\\final_dataset\\IMPMCT\\auxiliary information\\merge_IDs.txt', merge_IDs.ID.values.reshape(-1, 1), fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53787/53787 [00:01<00:00, 27308.01it/s]\n"
     ]
    }
   ],
   "source": [
    "ERA5_tracks_infos.isolated_area_path = [np.array(ast.literal_eval(line)) for line in ERA5_tracks_infos.isolated_area_path]\n",
    "ERA5_tracks_infos.original_vort_line = [np.array(ast.literal_eval(line)) for line in ERA5_tracks_infos.original_vort_line]\n",
    "record_line_area_r = []\n",
    "for isolated_area_path in tqdm(ERA5_tracks_infos.isolated_area_path.values):\n",
    "    line_area  = area({'type':'Polygon','coordinates':[isolated_area_path.tolist()]})/1e6########   闭环,平方公里[(纬度，经度)]\n",
    "    line_area_r = np.sqrt(line_area/np.pi)#涡旋的半径是由相同面积的圆的的半径\n",
    "    record_line_area_r.append(line_area_r)\n",
    "ERA5_tracks_infos['r'] = record_line_area_r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是否有重叠的轨迹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                 slp_lon  slp_lat\n",
       "2001-02-20 05:00:00  -2.25    70.25      1\n",
       "2014-02-28 18:00:00   48.25   79.50      1\n",
       "2014-02-28 16:00:00   48.50   79.75      1\n",
       "2014-02-28 15:00:00   48.75   79.75      1\n",
       "2014-02-28 14:00:00   49.00   79.75      1\n",
       "                                        ..\n",
       "2008-03-15 01:00:00   5.25    75.50      1\n",
       "2008-03-15 00:00:00   5.25    75.50      1\n",
       "2008-03-14 23:00:00   4.75    75.75      1\n",
       "2008-03-12 11:00:00  -22.50   59.50      1\n",
       "2023-11-07 06:00:00   51.50   83.75      1\n",
       "Name: count, Length: 33403, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ERA5_tracks_infos.value_counts(['time','slp_lon','slp_lat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "气旋大小特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据坐标计算点到直线距离\n",
    "def get_distance_from_point_to_line(x0,y0, line):\n",
    "    A = line[:,3]-line[:,1]#A=Y2-Y1\n",
    "    B = line[:,0]-line[:,2]#B=X1-X2\n",
    "    C = -(A * line[:,0] + B * line[:,1])#C=X1*(Y1-Y2)+Y1*(X2-X1)\n",
    "    #根据点到直线的距离公式计算距离\n",
    "    distance = np.abs(A * x0 + B * y0 + C) / (np.sqrt(A**2 + B**2))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#长度和宽度,x1-x4,y1,y4逆时针旋转\n",
    "w = np.sqrt((cloud_infos.x1-cloud_infos.x2)**2+(cloud_infos.y1-cloud_infos.y2)**2).values\n",
    "h= np.sqrt((cloud_infos.x1-cloud_infos.x4)**2+(cloud_infos.y1-cloud_infos.y4)**2).values\n",
    "r = np.concatenate([h.reshape((-1,1)),w.reshape((-1,1))],axis=1)\n",
    "#width和length分别为矩形短边和长边\n",
    "cloud_infos['width'],cloud_infos['length'] = np.min(r,axis=1)*2,np.max(r,axis=1)*2#KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算云中心至短边的长距离和短距离\n",
    "height_short_bool,width_short_bool= cloud_infos['width']==h*2,cloud_infos['width']==w*2\n",
    "short_line1 = cloud_infos.loc[height_short_bool,['x2','y2','x3','y3']].values\n",
    "short_line2 = cloud_infos.loc[height_short_bool,['x1','y1','x4','y4']].values\n",
    "short_line3 = cloud_infos.loc[width_short_bool,['x1','y1','x2','y2']].values\n",
    "short_line4 = cloud_infos.loc[width_short_bool,['x4','y4','x3','y3']].values\n",
    "dis1 = get_distance_from_point_to_line(cloud_infos.loc[height_short_bool,'x0'].values,cloud_infos.loc[height_short_bool,'y0'].values, short_line1)\n",
    "dis2 = get_distance_from_point_to_line(cloud_infos.loc[height_short_bool,'x0'].values,cloud_infos.loc[height_short_bool,'y0'].values, short_line2)\n",
    "dis3 = get_distance_from_point_to_line(cloud_infos.loc[width_short_bool,'x0'].values,cloud_infos.loc[width_short_bool,'y0'].values, short_line3)\n",
    "dis4 = get_distance_from_point_to_line(cloud_infos.loc[width_short_bool,'x0'].values,cloud_infos.loc[width_short_bool,'y0'].values, short_line4)\n",
    "cloud_infos.loc[height_short_bool,'dis1'] = dis1\n",
    "cloud_infos.loc[height_short_bool,'dis2'] = dis2\n",
    "cloud_infos.loc[width_short_bool,'dis1'] = dis3\n",
    "cloud_infos.loc[width_short_bool,'dis2'] = dis4\n",
    "cloud_infos['l_search'] = np.max(cloud_infos[['dis1','dis2']],axis=1)*2\n",
    "cloud_infos['s_search'] = np.min(cloud_infos[['dis1','dis2']],axis=1)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_describe = cloud_infos.value_counts('ID').reset_index().sort_values('ID')\n",
    "steps = np.concatenate(([np.arange(count)+1 for count in step_describe['count'].values]))\n",
    "cloud_infos['step'] = steps\n",
    "cloud_infos.drop(columns=['is_visible','dis1','dis2']).to_excel(r'D:\\Desktop\\final_dataset\\cloud_infos.xlsx')\n",
    "ERA5_tracks_infos.to_excel(r'D:\\Desktop\\final_dataset\\tracks.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_infos.drop(columns=['is_visible','dis1','dis2']).to_excel(r'D:\\Desktop\\final_dataset\\cloud_infos.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPMCT_tracks  = ERA5_tracks_infos.rename({'peak_value':'ζ_smth850'},axis=1)\n",
    "new_columns_order  = ['ID','time','lon','lat','ζ_smth850','r','slp_lon','slp_lat','slp']\n",
    "IMPMCT_tracks = IMPMCT_tracks[new_columns_order].reset_index(drop=True)\n",
    "IMPMCT_tracks[['ζ_smth850', 'r', 'slp_lon','slp']] = np.round(IMPMCT_tracks[['ζ_smth850', 'r', 'slp_lon','slp']],2)\n",
    "IMPMCT_tracks.to_excel(r\"D:\\Desktop\\final_dataset\\IMPMCT\\ERA5_tracks.xlsx\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理隔一个小时连接的轨迹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>time</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>ζ_smth850</th>\n",
       "      <th>r</th>\n",
       "      <th>slp_lon</th>\n",
       "      <th>slp_lat</th>\n",
       "      <th>slp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2001-02-20 02:00:00</td>\n",
       "      <td>-3.50</td>\n",
       "      <td>68.50</td>\n",
       "      <td>1.27</td>\n",
       "      <td>50.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2001-02-20 03:00:00</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>68.50</td>\n",
       "      <td>1.37</td>\n",
       "      <td>81.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2001-02-20 04:00:00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>68.75</td>\n",
       "      <td>1.47</td>\n",
       "      <td>97.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2001-02-20 05:00:00</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>69.50</td>\n",
       "      <td>1.65</td>\n",
       "      <td>113.22</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>70.25</td>\n",
       "      <td>988.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2001-02-20 06:00:00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>69.50</td>\n",
       "      <td>1.76</td>\n",
       "      <td>124.18</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>70.25</td>\n",
       "      <td>986.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53782</th>\n",
       "      <td>1185</td>\n",
       "      <td>2023-11-07 02:00:00</td>\n",
       "      <td>51.50</td>\n",
       "      <td>83.25</td>\n",
       "      <td>2.32</td>\n",
       "      <td>102.30</td>\n",
       "      <td>52.25</td>\n",
       "      <td>83.25</td>\n",
       "      <td>1002.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53783</th>\n",
       "      <td>1185</td>\n",
       "      <td>2023-11-07 03:00:00</td>\n",
       "      <td>51.50</td>\n",
       "      <td>83.50</td>\n",
       "      <td>2.30</td>\n",
       "      <td>103.10</td>\n",
       "      <td>52.25</td>\n",
       "      <td>83.25</td>\n",
       "      <td>1002.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53784</th>\n",
       "      <td>1185</td>\n",
       "      <td>2023-11-07 04:00:00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>83.75</td>\n",
       "      <td>2.22</td>\n",
       "      <td>103.82</td>\n",
       "      <td>52.25</td>\n",
       "      <td>83.50</td>\n",
       "      <td>1003.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53785</th>\n",
       "      <td>1185</td>\n",
       "      <td>2023-11-07 05:00:00</td>\n",
       "      <td>50.75</td>\n",
       "      <td>84.00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>103.44</td>\n",
       "      <td>52.00</td>\n",
       "      <td>83.75</td>\n",
       "      <td>1003.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53786</th>\n",
       "      <td>1185</td>\n",
       "      <td>2023-11-07 06:00:00</td>\n",
       "      <td>50.50</td>\n",
       "      <td>84.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>97.35</td>\n",
       "      <td>51.50</td>\n",
       "      <td>83.75</td>\n",
       "      <td>1003.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53787 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                time    lon    lat  ζ_smth850       r  slp_lon  \\\n",
       "0         1 2001-02-20 02:00:00  -3.50  68.50       1.27   50.26      NaN   \n",
       "1         1 2001-02-20 03:00:00  -1.75  68.50       1.37   81.97      NaN   \n",
       "2         1 2001-02-20 04:00:00  -0.75  68.75       1.47   97.21      NaN   \n",
       "3         1 2001-02-20 05:00:00  -0.25  69.50       1.65  113.22    -2.25   \n",
       "4         1 2001-02-20 06:00:00   1.50  69.50       1.76  124.18    -0.75   \n",
       "...     ...                 ...    ...    ...        ...     ...      ...   \n",
       "53782  1185 2023-11-07 02:00:00  51.50  83.25       2.32  102.30    52.25   \n",
       "53783  1185 2023-11-07 03:00:00  51.50  83.50       2.30  103.10    52.25   \n",
       "53784  1185 2023-11-07 04:00:00  51.00  83.75       2.22  103.82    52.25   \n",
       "53785  1185 2023-11-07 05:00:00  50.75  84.00       2.10  103.44    52.00   \n",
       "53786  1185 2023-11-07 06:00:00  50.50  84.00       2.00   97.35    51.50   \n",
       "\n",
       "       slp_lat      slp  \n",
       "0          NaN      NaN  \n",
       "1          NaN      NaN  \n",
       "2          NaN      NaN  \n",
       "3        70.25   988.02  \n",
       "4        70.25   986.56  \n",
       "...        ...      ...  \n",
       "53782    83.25  1002.35  \n",
       "53783    83.25  1002.51  \n",
       "53784    83.50  1003.06  \n",
       "53785    83.75  1003.23  \n",
       "53786    83.75  1003.68  \n",
       "\n",
       "[53787 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ERA5_tracks = pd.read_excel(r'D:\\Desktop\\final_dataset\\IMPMCT\\ERA5_tracks.xlsx')\n",
    "ERA5_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = []\n",
    "for ID in ERA5_tracks.ID.unique():\n",
    "    chosed_track = ERA5_tracks.loc[ERA5_tracks.ID==ID]\n",
    "    chosed_record = [chosed_track]\n",
    "    for j in range(len(chosed_track)-1):\n",
    "        delta_hour = (chosed_track.iloc[j+1]['time']-chosed_track.iloc[j]['time']).total_seconds()/3600\n",
    "        if delta_hour==2:\n",
    "            new_info = chosed_track.iloc[j:j+1].copy()\n",
    "            new_info[['slp_lon','slp_lat','slp']] = np.nan\n",
    "            new_info['time'] = chosed_track.iloc[j]['time']+timedelta(hours=1)\n",
    "            new_info[['lon','lat','ζ_smth850','r']] = np.mean(chosed_track.iloc[j:j+2][['lon','lat','ζ_smth850','r']],axis=0)\n",
    "            chosed_record.append(new_info)\n",
    "    record.append(pd.concat(chosed_record,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>time</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>ζ_smth850</th>\n",
       "      <th>r</th>\n",
       "      <th>slp_lon</th>\n",
       "      <th>slp_lat</th>\n",
       "      <th>slp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2001-02-20 02:00:00</td>\n",
       "      <td>-3.50</td>\n",
       "      <td>68.50</td>\n",
       "      <td>1.27</td>\n",
       "      <td>50.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2001-02-20 03:00:00</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>68.50</td>\n",
       "      <td>1.37</td>\n",
       "      <td>81.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2001-02-20 04:00:00</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>68.75</td>\n",
       "      <td>1.47</td>\n",
       "      <td>97.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2001-02-20 05:00:00</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>69.50</td>\n",
       "      <td>1.65</td>\n",
       "      <td>113.22</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>70.25</td>\n",
       "      <td>988.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2001-02-20 06:00:00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>69.50</td>\n",
       "      <td>1.76</td>\n",
       "      <td>124.18</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>70.25</td>\n",
       "      <td>986.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53782</th>\n",
       "      <td>1185</td>\n",
       "      <td>2023-11-07 02:00:00</td>\n",
       "      <td>51.50</td>\n",
       "      <td>83.25</td>\n",
       "      <td>2.32</td>\n",
       "      <td>102.30</td>\n",
       "      <td>52.25</td>\n",
       "      <td>83.25</td>\n",
       "      <td>1002.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53783</th>\n",
       "      <td>1185</td>\n",
       "      <td>2023-11-07 03:00:00</td>\n",
       "      <td>51.50</td>\n",
       "      <td>83.50</td>\n",
       "      <td>2.30</td>\n",
       "      <td>103.10</td>\n",
       "      <td>52.25</td>\n",
       "      <td>83.25</td>\n",
       "      <td>1002.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53784</th>\n",
       "      <td>1185</td>\n",
       "      <td>2023-11-07 04:00:00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>83.75</td>\n",
       "      <td>2.22</td>\n",
       "      <td>103.82</td>\n",
       "      <td>52.25</td>\n",
       "      <td>83.50</td>\n",
       "      <td>1003.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53785</th>\n",
       "      <td>1185</td>\n",
       "      <td>2023-11-07 05:00:00</td>\n",
       "      <td>50.75</td>\n",
       "      <td>84.00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>103.44</td>\n",
       "      <td>52.00</td>\n",
       "      <td>83.75</td>\n",
       "      <td>1003.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53786</th>\n",
       "      <td>1185</td>\n",
       "      <td>2023-11-07 06:00:00</td>\n",
       "      <td>50.50</td>\n",
       "      <td>84.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>97.35</td>\n",
       "      <td>51.50</td>\n",
       "      <td>83.75</td>\n",
       "      <td>1003.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53875 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                time    lon    lat  ζ_smth850       r  slp_lon  \\\n",
       "0         1 2001-02-20 02:00:00  -3.50  68.50       1.27   50.26      NaN   \n",
       "1         1 2001-02-20 03:00:00  -1.75  68.50       1.37   81.97      NaN   \n",
       "2         1 2001-02-20 04:00:00  -0.75  68.75       1.47   97.21      NaN   \n",
       "3         1 2001-02-20 05:00:00  -0.25  69.50       1.65  113.22    -2.25   \n",
       "4         1 2001-02-20 06:00:00   1.50  69.50       1.76  124.18    -0.75   \n",
       "...     ...                 ...    ...    ...        ...     ...      ...   \n",
       "53782  1185 2023-11-07 02:00:00  51.50  83.25       2.32  102.30    52.25   \n",
       "53783  1185 2023-11-07 03:00:00  51.50  83.50       2.30  103.10    52.25   \n",
       "53784  1185 2023-11-07 04:00:00  51.00  83.75       2.22  103.82    52.25   \n",
       "53785  1185 2023-11-07 05:00:00  50.75  84.00       2.10  103.44    52.00   \n",
       "53786  1185 2023-11-07 06:00:00  50.50  84.00       2.00   97.35    51.50   \n",
       "\n",
       "       slp_lat      slp  \n",
       "0          NaN      NaN  \n",
       "1          NaN      NaN  \n",
       "2          NaN      NaN  \n",
       "3        70.25   988.02  \n",
       "4        70.25   986.56  \n",
       "...        ...      ...  \n",
       "53782    83.25  1002.35  \n",
       "53783    83.25  1002.51  \n",
       "53784    83.50  1003.06  \n",
       "53785    83.75  1003.23  \n",
       "53786    83.75  1003.68  \n",
       "\n",
       "[53875 rows x 9 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ERA5_tracks = pd.concat(record,axis=0).sort_values(['ID','time'])\n",
    "ERA5_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 447/1185 [00:04<00:07, 104.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1185/1185 [00:12<00:00, 95.62it/s] \n"
     ]
    }
   ],
   "source": [
    "record = []\n",
    "for ID in tqdm(ERA5_tracks.ID.unique()):\n",
    "    chosed_track = ERA5_tracks.loc[ERA5_tracks.ID==ID]\n",
    "    chosed_record = [chosed_track]\n",
    "    for j in range(len(chosed_track)-1):\n",
    "        delta_hour = (chosed_track.iloc[j+1]['time']-chosed_track.iloc[j]['time']).total_seconds()/3600\n",
    "        if delta_hour>1:\n",
    "            print(ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID为436的轨迹相差3个时次,其在云图中连接,在ERA5中断开\n"
     ]
    }
   ],
   "source": [
    "print('ID为436的轨迹相差3个时次,其在云图中连接,在ERA5中断开')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERA5_tracks.to_excel(r'D:\\Desktop\\final_dataset\\IMPMCT\\ERA5_tracks.xlsx',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FRZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
